{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "### 필요한 패키지 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from class_DeepNetwork import DNN, DNN_decreasing, GCN, GCN_2\n",
    "from import_data import toSparse, NORMALIZATION_v2\n",
    "from configuration import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils import EarlyStopping\n",
    "import os\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "### GPU 사용 가능 여부, 랜덤 시드넘버 설정, 배차사이즈/GCN Layer 수/필터 수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's go CUDA!!!!!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Let's go CUDA!!!!!\")\n",
    "cuda = torch.device('cuda')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "np.random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_gem = 'basedata/GEM_train_fs_9074.csv'\n",
    "val_gem = 'basedata/GEM_val_fs_9074.csv'\n",
    "test_gem = 'basedata/GEM_test_fs_9074.csv'\n",
    "\n",
    "train_label = 'basedata/label_train_9074.csv'\n",
    "val_label = 'basedata/label_val_9074.csv'\n",
    "test_label = 'basedata/label_test_9074.csv'\n",
    "mode = 'standard'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "### Dataset, DataLoader 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GcnDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, train_path, gem_path, label_path, mode):\n",
    "        self.data = data\n",
    "        \n",
    "        # Node Feature Data(GEM); Train data 불러오기\n",
    "        train_gem = np.transpose(np.loadtxt(gem_path, dtype=str, delimiter=','))\n",
    "        self.train_gem, self.means, self.stds = NORMALIZATION_v2(train_gem[1:, 1:], mode)\n",
    "        \n",
    "        if self.data == 'train':\n",
    "            self.gem = self.train_gem\n",
    "\n",
    "            # Label Data 불러오기\n",
    "            train_label = np.loadtxt(label_path, dtype=str, delimiter=',')\n",
    "            train_label = np.transpose(train_label)[1]\n",
    "            self.label = np.array([int(float(i)) for i in train_label])[1:]\n",
    "\n",
    "            if not len(os.listdir('train')) == len(self.gem) & len(self.gem) == len(self.label):\n",
    "                print('Warning: 데이터 개수 불일치')\n",
    "                print('Train 파일 개수: %d' % (len(os.listdir('val'))))\n",
    "                print('GEM 개수: %d' % (len(self.gem)))\n",
    "                print('Label 개수: %d' % (len(self.label)))\n",
    "            else:\n",
    "                print('Train 데이터 개수 일치')\n",
    "                print('Size: {}'.format(self.gem.shape))\n",
    "                \n",
    "        elif self.data == 'val':\n",
    "            val_gem = np.transpose(np.loadtxt(gem_path, dtype=str, delimiter=','))\n",
    "            val_gem = np.array([i.astype(float) for i in val_gem[1:, 1:]])\n",
    "            self.gem = (val_gem - self.means) / self.stds\n",
    "\n",
    "            # Label Data 불러오기\n",
    "            val_label = np.loadtxt(label_path, dtype=str, delimiter=',')\n",
    "            val_label = np.transpose(val_label)[1]\n",
    "            self.label = np.array([int(float(i)) for i in val_label])[1:]\n",
    "            \n",
    "            if not len(os.listdir('val')) == len(self.gem) & len(self.gem) == len(self.label):\n",
    "                print('Warning: 데이터 개수 불일치')\n",
    "                print('Val 파일 개수: %d' % (len(os.listdir('val'))))\n",
    "                print('GEM 개수: %d' % (len(self.gem)))\n",
    "                print('Label 개수: %d' % (len(self.label)))\n",
    "            else:\n",
    "                print('Val 데이터 개수 일치')\n",
    "                print('Size: {}'.format(self.gem.shape))\n",
    "        \n",
    "        elif self.data == 'test':\n",
    "            test_gem = np.transpose(np.loadtxt(gem_path, dtype=str, delimiter=','))\n",
    "            test_gem = np.array([i.astype(float) for i in test_gem[1:, 1:]])\n",
    "            self.gem = (test_gem - self.means) / self.stds\n",
    "\n",
    "            # Label Data 불러오기\n",
    "            test_label = np.loadtxt(label_path, dtype=str, delimiter=',')\n",
    "            test_label = np.transpose(test_label)[1]\n",
    "            self.label = np.array([int(float(i)) for i in test_label])[1:]\n",
    "            \n",
    "            if not len(os.listdir('test')) == len(self.gem) & len(self.gem) == len(self.label):\n",
    "                print('Warning: 데이터 개수 불일치')\n",
    "                print('Test 파일 개수: %d' % (len(os.listdir('test'))))\n",
    "                print('GEM 개수: %d' % (len(self.gem)))\n",
    "                print('Label 개수: %d' % (len(self.label)))\n",
    "            else:\n",
    "                print('Test 데이터 개수 일치')\n",
    "                print('Size: {}'.format(self.gem.shape))\n",
    "                \n",
    "        self.gem = torch.Tensor(self.gem)\n",
    "        self.label = torch.LongTensor(self.label)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.data + '/' + self.data + '_' + str(idx) + '.npz'\n",
    "        support = sparse.load_npz(path)\n",
    "        support = toSparse(support)\n",
    "\n",
    "        return (self.gem[idx], support, self.label[idx])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 데이터 개수 일치\n",
      "Size: (625, 9074)\n",
      "Val 데이터 개수 일치\n",
      "Size: (157, 9074)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = GcnDataset('train', train_gem, train_gem, train_label, mode)\n",
    "val_dataset = GcnDataset('val', train_gem, val_gem, val_label, mode)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  \n",
    "### Modeling\n",
    "1. Feature 및 Label 개수 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = len(np.loadtxt(val_gem, dtype=str, delimiter=',')) - 1\n",
    "num_classes = len(list(set(np.loadtxt(val_label, dtype=str, delimiter=',')[1:, 1])))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Parameter Setting: 최대 에폭, Loss 함수, Optimizer, GCN Layer 개수, filter 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCH = 1000\n",
    "\n",
    "def Parameters(net):\n",
    "    \"\"\"\n",
    "        신경망으로부터 파라미터 받아오는 함수\n",
    "\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    return criterion, optimizer\n",
    "\n",
    "num_layers = 1\n",
    "num_filter = 4\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 0.54089\n",
      "Train_loss: 0.48375\n",
      "Train_loss: 0.43404\n",
      "Train_loss: 0.39409\n",
      "Train_loss: 0.44311\n",
      "Train_loss: 0.42865\n",
      "Train_loss: 0.41418\n",
      "Train_loss: 0.40642\n",
      "Train_loss: 0.39394\n",
      "Train_loss: 0.38038\n",
      "Train_loss: 0.36531\n",
      "Train_loss: 0.36059\n",
      "Train_loss: 0.34755\n",
      "Train_loss: 0.34835\n",
      "Train_loss: 0.35321\n",
      "Train_loss: 0.34539\n",
      "Train_loss: 0.33640\n",
      "Train_loss: 0.34035\n",
      "Train_loss: 0.33762\n",
      "Train_loss: 0.33106\n",
      "Epoch:     0 / Train Loss: 0.33106 / Val Loss: 0.02728 / Val Acc: 0.745\n",
      "Train_loss: 0.11530\n",
      "Train_loss: 0.14357\n",
      "Train_loss: 0.17391\n",
      "Train_loss: 0.18086\n",
      "Train_loss: 0.18251\n",
      "Train_loss: 0.16892\n",
      "Train_loss: 0.16569\n",
      "Train_loss: 0.15790\n",
      "Train_loss: 0.16136\n",
      "Train_loss: 0.15863\n",
      "Train_loss: 0.15745\n",
      "Train_loss: 0.15418\n",
      "Train_loss: 0.15782\n",
      "Train_loss: 0.15901\n",
      "Train_loss: 0.15906\n",
      "Train_loss: 0.16097\n",
      "Train_loss: 0.15876\n",
      "Train_loss: 0.15562\n",
      "Train_loss: 0.15234\n",
      "Train_loss: 0.15371\n",
      "Epoch:     1 / Train Loss: 0.15371 / Val Loss: 0.02210 / Val Acc: 0.764\n",
      "Train_loss: 0.10002\n",
      "Train_loss: 0.09105\n",
      "Train_loss: 0.09462\n",
      "Train_loss: 0.08993\n",
      "Train_loss: 0.09550\n",
      "Train_loss: 0.09258\n",
      "Train_loss: 0.09522\n",
      "Train_loss: 0.09546\n",
      "Train_loss: 0.09576\n",
      "Train_loss: 0.09451\n",
      "Train_loss: 0.09325\n",
      "Train_loss: 0.09419\n",
      "Train_loss: 0.09472\n",
      "Train_loss: 0.09464\n",
      "Train_loss: 0.09471\n",
      "Train_loss: 0.09359\n",
      "Train_loss: 0.09244\n",
      "Train_loss: 0.09257\n",
      "Train_loss: 0.09219\n",
      "Train_loss: 0.09223\n",
      "Epoch:     2 / Train Loss: 0.09223 / Val Loss: 0.02317 / Val Acc: 0.815\n",
      "Train_loss: 0.08685\n",
      "Train_loss: 0.06988\n",
      "Train_loss: 0.06574\n",
      "Train_loss: 0.06637\n",
      "Train_loss: 0.06883\n",
      "Train_loss: 0.07550\n",
      "Train_loss: 0.08110\n",
      "Train_loss: 0.07654\n",
      "Train_loss: 0.07904\n",
      "Train_loss: 0.08048\n",
      "Train_loss: 0.07877\n",
      "Train_loss: 0.07703\n",
      "Train_loss: 0.08024\n",
      "Train_loss: 0.08019\n",
      "Train_loss: 0.08257\n",
      "Train_loss: 0.08139\n",
      "Train_loss: 0.08021\n",
      "Train_loss: 0.07981\n",
      "Train_loss: 0.07922\n",
      "Train_loss: 0.08110\n",
      "Epoch:     3 / Train Loss: 0.08110 / Val Loss: 0.01990 / Val Acc: 0.790\n",
      "Train_loss: 0.05645\n",
      "Train_loss: 0.06308\n",
      "Train_loss: 0.07074\n",
      "Train_loss: 0.06708\n",
      "Train_loss: 0.06658\n",
      "Train_loss: 0.06245\n",
      "Train_loss: 0.06270\n",
      "Train_loss: 0.06239\n",
      "Train_loss: 0.06343\n",
      "Train_loss: 0.05940\n",
      "Train_loss: 0.06155\n",
      "Train_loss: 0.06111\n",
      "Train_loss: 0.06007\n",
      "Train_loss: 0.05933\n",
      "Train_loss: 0.05864\n",
      "Train_loss: 0.05835\n",
      "Train_loss: 0.05836\n",
      "Train_loss: 0.05809\n",
      "Train_loss: 0.05744\n",
      "Train_loss: 0.05585\n",
      "Epoch:     4 / Train Loss: 0.05585 / Val Loss: 0.02020 / Val Acc: 0.822\n",
      "Train_loss: 0.04030\n",
      "Train_loss: 0.05235\n",
      "Train_loss: 0.04769\n",
      "Train_loss: 0.04435\n",
      "Train_loss: 0.04413\n",
      "Train_loss: 0.04336\n",
      "Train_loss: 0.04269\n",
      "Train_loss: 0.04167\n",
      "Train_loss: 0.04367\n",
      "Train_loss: 0.04318\n",
      "Train_loss: 0.04236\n",
      "Train_loss: 0.04232\n",
      "Train_loss: 0.04338\n",
      "Train_loss: 0.04326\n",
      "Train_loss: 0.04267\n",
      "Train_loss: 0.04302\n",
      "Train_loss: 0.04236\n",
      "Train_loss: 0.04156\n",
      "Train_loss: 0.04193\n",
      "Train_loss: 0.04125\n",
      "Epoch:     5 / Train Loss: 0.04125 / Val Loss: 0.01945 / Val Acc: 0.815\n",
      "Train_loss: 0.03787\n",
      "Train_loss: 0.03670\n",
      "Train_loss: 0.03523\n",
      "Train_loss: 0.03261\n",
      "Train_loss: 0.03467\n",
      "Train_loss: 0.03422\n",
      "Train_loss: 0.03438\n",
      "Train_loss: 0.03295\n",
      "Train_loss: 0.03316\n",
      "Train_loss: 0.03332\n",
      "Train_loss: 0.03195\n",
      "Train_loss: 0.03214\n",
      "Train_loss: 0.03217\n",
      "Train_loss: 0.03367\n",
      "Train_loss: 0.03366\n",
      "Train_loss: 0.03471\n",
      "Train_loss: 0.03398\n",
      "Train_loss: 0.03509\n",
      "Train_loss: 0.03505\n",
      "Train_loss: 0.03577\n",
      "Epoch:     6 / Train Loss: 0.03577 / Val Loss: 0.02130 / Val Acc: 0.822\n",
      "Train_loss: 0.02541\n",
      "Train_loss: 0.02561\n",
      "Train_loss: 0.03365\n",
      "Train_loss: 0.03454\n",
      "Train_loss: 0.03508\n",
      "Train_loss: 0.03349\n",
      "Train_loss: 0.03326\n",
      "Train_loss: 0.03290\n",
      "Train_loss: 0.03246\n",
      "Train_loss: 0.03107\n",
      "Train_loss: 0.03000\n",
      "Train_loss: 0.03035\n",
      "Train_loss: 0.02960\n",
      "Train_loss: 0.02958\n",
      "Train_loss: 0.02895\n",
      "Train_loss: 0.03049\n",
      "Train_loss: 0.03119\n",
      "Train_loss: 0.03059\n",
      "Train_loss: 0.03018\n",
      "Train_loss: 0.03022\n",
      "Epoch:     7 / Train Loss: 0.03022 / Val Loss: 0.01974 / Val Acc: 0.828\n",
      "Train_loss: 0.02401\n",
      "Train_loss: 0.02450\n",
      "Train_loss: 0.02235\n",
      "Train_loss: 0.02793\n",
      "Train_loss: 0.02650\n",
      "Train_loss: 0.02545\n",
      "Train_loss: 0.02627\n",
      "Train_loss: 0.02528\n",
      "Train_loss: 0.02501\n",
      "Train_loss: 0.02475\n",
      "Train_loss: 0.02609\n",
      "Train_loss: 0.02590\n",
      "Train_loss: 0.02547\n",
      "Train_loss: 0.02487\n",
      "Train_loss: 0.02526\n",
      "Train_loss: 0.02537\n",
      "Train_loss: 0.02521\n",
      "Train_loss: 0.02554\n",
      "Train_loss: 0.02527\n",
      "Train_loss: 0.02491\n",
      "Epoch:     8 / Train Loss: 0.02491 / Val Loss: 0.02083 / Val Acc: 0.809\n",
      "Train_loss: 0.02461\n",
      "Train_loss: 0.01640\n",
      "Train_loss: 0.01727\n",
      "Train_loss: 0.01682\n",
      "Train_loss: 0.01640\n",
      "Train_loss: 0.01684\n",
      "Train_loss: 0.01655\n",
      "Train_loss: 0.01692\n",
      "Train_loss: 0.01777\n",
      "Train_loss: 0.01834\n",
      "Train_loss: 0.01987\n",
      "Train_loss: 0.02092\n",
      "Train_loss: 0.02115\n",
      "Train_loss: 0.02100\n",
      "Train_loss: 0.02091\n",
      "Train_loss: 0.02134\n",
      "Train_loss: 0.02103\n",
      "Train_loss: 0.02091\n",
      "Train_loss: 0.02095\n",
      "Train_loss: 0.02092\n",
      "Epoch:     9 / Train Loss: 0.02092 / Val Loss: 0.02042 / Val Acc: 0.815\n",
      "Train_loss: 0.01633\n",
      "Train_loss: 0.01979\n",
      "Train_loss: 0.01802\n",
      "Train_loss: 0.01759\n",
      "Train_loss: 0.01554\n",
      "Train_loss: 0.01575\n",
      "Train_loss: 0.01478\n",
      "Train_loss: 0.01904\n",
      "Train_loss: 0.01861\n",
      "Train_loss: 0.01828\n",
      "Train_loss: 0.01808\n",
      "Train_loss: 0.01778\n",
      "Train_loss: 0.01823\n",
      "Train_loss: 0.01801\n",
      "Train_loss: 0.01885\n",
      "Train_loss: 0.01899\n",
      "Train_loss: 0.01876\n",
      "Train_loss: 0.01847\n",
      "Train_loss: 0.01827\n",
      "Train_loss: 0.01858\n",
      "Epoch:    10 / Train Loss: 0.01858 / Val Loss: 0.02120 / Val Acc: 0.828\n",
      "Early Stopping Validated\n"
     ]
    }
   ],
   "source": [
    "net = GCN(num_vars, 1, num_classes, num_filter)\n",
    "criterion, optimizer = Parameters(net)\n",
    "net.to(cuda)\n",
    "\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    torch.cuda.empty_cache()\n",
    "    if epoch == 0:\n",
    "        early_stopping = EarlyStopping(patience=patience, path='gcn1.pt', verbose=False)\n",
    "    else:\n",
    "        early_stopping = EarlyStopping(patience=patience, best_score=best_score, counter=counter, path='gcn1.pt', verbose=False)\n",
    "    \n",
    "    net.train()\n",
    "    train_loss = 0.\n",
    "    tcnt = 0\n",
    "    for k, data in enumerate(train_dataloader):\n",
    "        inputs, support, labels = data\n",
    "        inputs = inputs.unsqueeze(-1)\n",
    "\n",
    "        inputs = inputs.to(cuda)\n",
    "        support = support.to(cuda)\n",
    "        labels = labels.to(cuda)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net((inputs, support))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tcnt += len(data)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        print('Train_loss: %.5f' % (train_loss / tcnt))\n",
    "        \n",
    "    net.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    val_loss = 0\n",
    "    vcnt = 0 \n",
    "    for k, data in enumerate(val_dataloader):\n",
    "        inputs, support, labels = data\n",
    "        inputs = inputs.unsqueeze(-1)\n",
    "\n",
    "        inputs = inputs.to(cuda)\n",
    "        labels = labels.to(cuda)\n",
    "\n",
    "        outputs = net((inputs, support))\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "        vcnt += len(labels)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print('Epoch: %5d / Train Loss: %5.5f / Val Loss: %5.5f / Val Acc: %5.3f'\n",
    "          % (epoch, train_loss / tcnt, val_loss / vcnt, correct / vcnt))\n",
    "    \n",
    "    best_score, counter, finish = early_stopping(val_loss / vcnt, net)\n",
    "    if finish:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
